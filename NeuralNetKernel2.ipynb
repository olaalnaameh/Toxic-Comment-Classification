{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Embedding, Input,GRU\n",
    "from keras.layers import Conv1D, MaxPooling1D, GlobalMaxPool1D, Dropout, concatenate\n",
    "from keras.preprocessing import text, sequence\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_features = 20000\n",
    "maxlen = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "#train = pickle.load(open('train.p','rb')).sample(10000)\n",
    "#test = pickle.load(open('test.p','rb')).sample(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_sentences_train = train[\"comment_text\"].fillna(\"_NaN_\").values\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values\n",
    "list_sentences_test = test.fillna(\"_NaN_\").values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "# train data\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "X_t = sequence.pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "# test data\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "X_te = sequence.pad_sequences(list_tokenized_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import theano.ifelse\n",
    "def cnn_rnn():\n",
    "    embed_size = 256\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    main = Embedding(max_features, embed_size)(inp)\n",
    "    main = Dropout(0.2)(main)\n",
    "    main = Conv1D(filters=32, kernel_size=2, padding='same', activation='relu')(main)\n",
    "    main = MaxPooling1D(pool_size=2)(main)\n",
    "    main = Conv1D(filters=32, kernel_size=2, padding='same', activation='relu')(main)\n",
    "    main = MaxPooling1D(pool_size=2)(main)\n",
    "    main = GRU(32)(main)\n",
    "    main = Dense(16, activation=\"relu\")(main)\n",
    "    main = Dense(6, activation=\"sigmoid\")(main)\n",
    "    model = Model(inputs=inp, outputs=main)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()     \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cnn_rnn()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print('Positive Labels ')\n",
    "any_category_positive = np.sum(y,1)\n",
    "print(pd.value_counts(any_category_positive))\n",
    "X_t_train, X_t_test, y_train, y_test = train_test_split(X_t, y, \n",
    "                                                        test_size = 0.10, \n",
    "                                                        )\n",
    "print('Training:', X_t_train.shape)\n",
    "print('Testing:', X_t_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_generator(X, y, batch_size, samplesN):\n",
    "    number_of_batches = samplesN/batch_size\n",
    "    counter=0\n",
    "    shuffle_index = np.arange(np.shape(y)[0])\n",
    "    np.random.shuffle(shuffle_index)\n",
    "    X =  X[shuffle_index, :]\n",
    "    y =  y[shuffle_index]\n",
    "    while 1:\n",
    "        index_batch = shuffle_index[batch_size*counter:batch_size*(counter+1)]\n",
    "        X_batch = X[index_batch,:].todense()\n",
    "        X_batch = np.expand_dims(X_batch, axis=2)\n",
    "        y_batch = y[index_batch]\n",
    "        counter += 1\n",
    "        yield(np.array(X_batch),y_batch)\n",
    "        if (counter < number_of_batches):\n",
    "            np.random.shuffle(shuffle_index)\n",
    "            counter=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-4d366ba87394>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mifelse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train.p'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mbatchSize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36m_new_Index\u001b[1;34m(cls, d)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m \u001b[1;32mdef\u001b[0m \u001b[0m_new_Index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m     \"\"\" This is called upon unpickling, rather than the default which doesn't\n\u001b[0;32m     96\u001b[0m     \u001b[0mhave\u001b[0m \u001b[0marguments\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mbreaks\u001b[0m \u001b[0m__new__\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#sparse matrix input keras\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy import sparse\n",
    "from keras import layers\n",
    "from keras.layers import Dense, Embedding, Input,GRU, Flatten, AveragePooling1D,Bidirectional\n",
    "from keras.layers import Conv2D,Conv1D, MaxPooling1D, GlobalMaxPool1D, Dropout, concatenate\n",
    "import theano.ifelse\n",
    "import numpy as np\n",
    "train = pickle.load(open('train.p','rb'))\n",
    "train=train[0:300]\n",
    "batchSize=50\n",
    "featuresN=50\n",
    "model = Sequential()\n",
    "print(model)\n",
    "\n",
    "model.add(Conv1D(64, 4, activation='relu', padding='causal',input_shape=(featuresN,1)))\n",
    "model.add(MaxPooling1D(4))\n",
    "model.add(Bidirectional(GRU(60, return_sequences=True,name='lstm_layer',dropout=0.2,recurrent_dropout=0.2)))\n",
    "model.add(MaxPooling1D(2))\n",
    "model.add(Conv1D(64, 4, activation='relu', padding='causal'))\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(layers.Dense(32,activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Generate dummy data\n",
    "\n",
    "\n",
    "#submission_logreg = pd.read_csv('sample_submission.csv')\n",
    "#submission_rf = pd.read_csv('sample_submission.csv')\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,2),max_features=featuresN)\n",
    "features = tfidf.fit_transform(train['comment_text'])\n",
    "print(features.shape)\n",
    "#test = tfidf.transform(test)\n",
    "\n",
    "#for col in ['toxic','severe_toxic','obscene','threat','insult','identity_hate']:\n",
    "#    print(col)\n",
    "#data=features.todense()\n",
    "data=features\n",
    "#data = np.expand_dims(data, axis=2)\n",
    "X_train_sparse = sparse.csr_matrix(data)\n",
    "print(X_train_sparse.shape[0])\n",
    "labels=np.array(train['toxic'].tolist())#train[col].tolist()\n",
    "\n",
    "model.fit_generator(generator=batch_generator(X_train_sparse, labels, batchSize, X_train_sparse.shape[0]),\n",
    "                    nb_epoch=10, \n",
    "                    steps_per_epoch=(X_train_sparse.shape[0]/batchSize))\n",
    "#model.fit(data, labels, epochs=10, batch_size=100)\n",
    "\n",
    "#submission.to_csv('simple_LogRegClassWeight.csv', index=False)\n",
    "\n",
    "#data = np.random.random((1000, 100))\n",
    "#labels = np.random.randint(2, size=(1000, 1))\n",
    "\n",
    "# Train the model, iterating on the data in batches of 32 samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n"
     ]
    }
   ],
   "source": [
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.450473677899\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "X_train_sparse = sparse.csr_matrix(data)\n",
    "#X_train_sparse = np.todense(X_train_sparse)[np.newaxis,:,:]\n",
    "preds = model.predict_generator(generator=batch_generator(X_train_sparse, labels, batchSize, X_train_sparse.shape[0]),\n",
    "                               steps=(X_train_sparse.shape[0]/batchSize))#np.expand_dims(X_train_sparse, axis=2))\n",
    "print(roc_auc_score(train['toxic'], preds))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "269"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.tolist().count(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.tolist().count(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
