{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import pickle\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "import theano.ifelse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "train = pickle.load(open('train.p','rb'))\n",
    "test = pickle.load(open('test.p','rb'))\n",
    "sentences = train['comment_text']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n"
     ]
    }
   ],
   "source": [
    "sent=[]\n",
    "for item in sentences:\n",
    "    sent.append(item.split())\n",
    "print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['messi', 'is', 'better', 'than', 'ronaldo'], ['microsoft', 'is', 'better', 'than', 'yahoo'], ['i', 'am', 'better', 'than', 'you'], ['messi', 'said', 'that', 'i', 'am', 'better', 'than', 'ronaldo']]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23703820.0\n"
     ]
    }
   ],
   "source": [
    "model = Word2Vec(iter = 1, size=300, window=6, min_count=0, workers=6, max_vocab_size=None,negative=10)\n",
    "model.build_vocab(sent)\n",
    "model.train(sent,total_examples=model.corpus_count,epochs=10,word_count=0,compute_loss=True)\n",
    "print(model.get_latest_training_loss())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('vuvuzelas', 0.44712678), ('misterwiki', 0.26793331), ('niggas', 0.11126363), ('off', 0.050557092), ('kike', 0.022720389), ('fuck', 0.020840088), ('cunt', 0.016637523), ('retard', 0.011730277), ('asshole', 0.0070780991), ('motherfucker', 0.0038134861)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ola.alnaameh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('hindus', 0.7804931402206421),\n",
       " ('shia', 0.7546062469482422),\n",
       " ('sunni', 0.7144185304641724),\n",
       " ('arab', 0.7045628428459167),\n",
       " ('islam', 0.7011244297027588),\n",
       " ('hindu', 0.652392566204071),\n",
       " ('jews', 0.6519992351531982),\n",
       " ('islamic', 0.6493295431137085),\n",
       " ('kurd', 0.64585280418396),\n",
       " ('nonmuslim', 0.6434457302093506)]"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model.predict_output_word(['fuck']))\n",
    "model.most_similar('muslim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tensor = []  \n",
    "ii=0\n",
    "for item in sent:\n",
    "    comm = [model.wv[ii] for ii in item if ii in model.wv.vocab] \n",
    "    tensor.append(comm)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(len(tensor[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.models.Sequential object at 0x000002065345CB00>\n",
      "(46, 300, 1)\n",
      "[[ 1.]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input arrays should have the same number of samples as target arrays. Found 46 input samples and 1 target samples.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-196-6950a98f8ab6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;31m#print(np.expand_dims(np.array(data), axis=2).shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m     \u001b[0mii\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mii\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m '''model.fit_generator(generator=batch_generator(X_train_sparse, labels, batchSize, X_train_sparse.shape[0]),\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m    891\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    892\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 893\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    895\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1553\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1554\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1555\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m   1556\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1557\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[0;32m   1419\u001b[0m                           \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mref\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1420\u001b[0m                           in zip(y, sample_weights, class_weights, self._feed_sample_weight_modes)]\n\u001b[1;32m-> 1421\u001b[1;33m         \u001b[0m_check_array_lengths\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1422\u001b[0m         _check_loss_and_target_compatibility(y,\n\u001b[0;32m   1423\u001b[0m                                              \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_feed_loss_fns\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_check_array_lengths\u001b[1;34m(inputs, targets, weights)\u001b[0m\n\u001b[0;32m    249\u001b[0m                          \u001b[1;34m'the same number of samples as target arrays. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m                          \u001b[1;34m'Found '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' input samples '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 251\u001b[1;33m                          'and ' + str(list(set_y)[0]) + ' target samples.')\n\u001b[0m\u001b[0;32m    252\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset_w\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    253\u001b[0m         raise ValueError('All sample_weight arrays should have '\n",
      "\u001b[1;31mValueError\u001b[0m: Input arrays should have the same number of samples as target arrays. Found 46 input samples and 1 target samples."
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy import sparse\n",
    "from keras import layers\n",
    "from keras.layers import Dense, Embedding, Input,GRU, Flatten, AveragePooling1D,Bidirectional\n",
    "from keras.layers import Conv2D,Conv1D, MaxPooling1D, GlobalMaxPool1D, Dropout, concatenate\n",
    "\n",
    "import numpy as np\n",
    "#train = pickle.load(open('train.p','rb'))\n",
    "#train=train[0:50000]\n",
    "#batchSize=500\n",
    "#featuresN=4000\n",
    "model = Sequential()\n",
    "print(model)\n",
    "\n",
    "model.add(Conv1D(64, 3, activation='relu', padding='same',input_shape=(300, 1)))\n",
    "model.add(GlobalMaxPool1D())\n",
    "# model.add(Bidirectional(GRU(60, return_sequences=True,name='lstm_layer',dropout=0.2,recurrent_dropout=0.2)))\n",
    "# model.add(MaxPooling1D(2))\n",
    "# model.add(Conv1D(64, 4, activation='relu', padding='causal'))\n",
    "# model.add(GlobalMaxPool1D())\n",
    "#model.add(layers.Dense(32,activation='softmax'))\n",
    "model.add(Dropout(0.5))\n",
    "#model.add(layers.Dense(32,activation='softmax'))\n",
    "model.add(Dense(1, activation='softmax'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Generate dummy data\n",
    "\n",
    "#tfidf = TfidfVectorizer(ngram_range=(1,2),max_features=featuresN)\n",
    "#features = tfidf.fit_transform(train['comment_text'])\n",
    "#print(features.shape)\n",
    "\n",
    "#data=tensor\n",
    "from keras.utils.np_utils import to_categorical\n",
    "ii = 0\n",
    "for item in tensor:\n",
    "#data = tensor#np.expand_dims(item, axis=2)\n",
    "#X_train_sparse = sparse.csr_matrix(data)\n",
    "\n",
    "#print(X_train_sparse.shape[0])\n",
    "    labels=np.array(train['toxic'].tolist())#train[col].tolist()\n",
    "    #print(len(item))\n",
    "    #print(np.array(item).shape)\n",
    "    #print()\n",
    "    item=np.expand_dims(item, axis=2)\n",
    "    print(item.shape)\n",
    "    label=to_categorical(labels[ii])\n",
    "    label=label.reshape(-1, 1)\n",
    "    print(label)\n",
    "#print(np.expand_dims(np.array(data), axis=2).shape)\n",
    "    model.fit(item, label, epochs=10, batch_size=100)\n",
    "    ii = ii+1\n",
    "'''model.fit_generator(generator=batch_generator(X_train_sparse, labels, batchSize, X_train_sparse.shape[0]),\n",
    "                    nb_epoch=10, \n",
    "                    steps_per_epoch=(X_train_sparse.shape[0]/batchSize))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 5 6 4 2 5 5 1 1 5 1 1 5 3 7 4 4 1 4 0]\n",
      "[[ 0.  0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import randint\n",
    "from numpy import argmax\n",
    "from keras.utils.np_utils import to_categorical\n",
    "k = 8\n",
    "n = 20\n",
    "x = randint(0, k, (n,))\n",
    "print(x)\n",
    "print(to_categorical(x, k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45086"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34, 300)\n",
      "['and', 'i', 'really', 'do', 'not', 'think', '-PRON-', 'understand', 'i', 'come', 'here', 'and', '-PRON-', 'idea', 'be', 'bad', 'right', 'away', 'what', 'kind', 'of', 'community', 'go', '-PRON-', 'have', 'bad', 'idea', 'go', 'away', 'instead', 'of', 'help', 'rewrite', '-PRON-']\n"
     ]
    }
   ],
   "source": [
    "print(comm.shape)\n",
    "print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[] in sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent.count([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy import sparse\n",
    "from keras import layers\n",
    "from keras.layers import Dense, Embedding, Input,GRU, Flatten, AveragePooling1D,Bidirectional\n",
    "from keras.layers import Conv2D,Conv1D, MaxPooling1D, GlobalMaxPool1D, Dropout, concatenate\n",
    "\n",
    "import numpy as np\n",
    "#train = pickle.load(open('train.p','rb'))\n",
    "#train=train[0:50000]\n",
    "#batchSize=500\n",
    "#featuresN=4000\n",
    "model = Sequential()\n",
    "print(model)\n",
    "\n",
    "model.add(Conv1D(64, 3, activation='relu', padding='same',input_shape=(None, 1)))\n",
    "model.add(MaxPooling1D(4))\n",
    "# model.add(Bidirectional(GRU(60, return_sequences=True,name='lstm_layer',dropout=0.2,recurrent_dropout=0.2)))\n",
    "# model.add(MaxPooling1D(2))\n",
    "# model.add(Conv1D(64, 4, activation='relu', padding='causal'))\n",
    "# model.add(GlobalMaxPool1D())\n",
    "#model.add(layers.Dense(32,activation='softmax'))\n",
    "model.add(Dropout(0.5))\n",
    "#model.add(layers.Dense(32,activation='softmax'))\n",
    "model.add(Dense(1, activation='softmax'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Generate dummy data\n",
    "\n",
    "#tfidf = TfidfVectorizer(ngram_range=(1,2),max_features=featuresN)\n",
    "#features = tfidf.fit_transform(train['comment_text'])\n",
    "#print(features.shape)\n",
    "\n",
    "#data=tensor\n",
    "from keras.utils.np_utils import to_categorical\n",
    "ii = 0\n",
    "#for item in tensor:\n",
    "#data = tensor#np.expand_dims(item, axis=2)\n",
    "#X_train_sparse = sparse.csr_matrix(data)\n",
    "\n",
    "#print(X_train_sparse.shape[0])\n",
    "labels=np.array(train['toxic'].tolist())#train[col].tolist()\n",
    "#print(len(item))\n",
    "#print(np.array(item).shape)\n",
    "#print()\n",
    "item=np.expand_dims(item, axis=2)\n",
    "print(item.shape)\n",
    "label=to_categorical(labels[ii])\n",
    "print(label.shape)\n",
    "label=label.reshape(-1, 1)\n",
    "print(label.shape)\n",
    "#print(np.expand_dims(np.array(data), axis=2).shape)\n",
    "model.fit(item, label, epochs=10, batch_size=100)\n",
    "ii = ii+1\n",
    "'''model.fit_generator(generator=batch_generator(X_train_sparse, labels, batchSize, X_train_sparse.shape[0]),\n",
    "                nb_epoch=10, \n",
    "                steps_per_epoch=(X_train_sparse.shape[0]/batchSize))\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
