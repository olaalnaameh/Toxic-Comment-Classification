{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from mlxtend.classifier import StackingClassifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    def __init__(self,features):\n",
    "        '''Constructor of the class\n",
    "            Args:\n",
    "                features(matrix) - feature matrix where the columns describe the features and the rows the samples\n",
    "        '''\n",
    "        self.features=features\n",
    "        \n",
    "    def set_labels(self,labels):\n",
    "        '''Method which sets the labels, i.e. the targets we want to predict based on the features\n",
    "            Args:\n",
    "                labels(vector) - vector where the i-th element is the label which corresponds to the i-th row of the features\n",
    "                matrix\n",
    "        '''\n",
    "        self.labels=labels\n",
    "        # the code below implements one-hot encoding of the labels\n",
    "        lb=LabelBinarizer()\n",
    "        lb.fit([label[0] for label in labels])\n",
    "        self.labels_one_hot=lb.transform(labels)\n",
    "\n",
    "    def get_train_test_split(self,test_size=0.2):\n",
    "        '''Method which splits the features and labels in a training set and a testing set\n",
    "            Args:\n",
    "                test_size(number) - defines the size of the test set\n",
    "        '''\n",
    "        #print(self.features,self.labels_one_hot)\n",
    "        X_train,X_test,y_train,y_test= train_test_split(self.features,self.labels_one_hot,test_size=test_size)\n",
    "        return (X_train,X_test,y_train,y_test)\n",
    "        \n",
    "    def get_batches(self,batch_size):\n",
    "        '''Helper method to devide the features and labels in mini-batches\n",
    "            Args:\n",
    "                batch_size(number) - size of the batches e.g. 256 (should fit in memory of the machine)\n",
    "        '''\n",
    "        assert len(self.features) == len(self.labels)\n",
    "        output_batches = []\n",
    "    \n",
    "        sample_size = len(self.features)\n",
    "        for start_i in range(0, sample_size, batch_size):\n",
    "            end_i = start_i + batch_size\n",
    "            batch = [self.features[start_i:end_i], self.labels_one_hot[start_i:end_i]]\n",
    "            output_batches.append(batch)\n",
    "        \n",
    "        return output_batches\n",
    "    \n",
    "    def build_neural_net(self,n_hidden_nodes):\n",
    "        '''Method which builds the Neuronal Network in TensorFlow\n",
    "            Args:\n",
    "                n_hidden_nodes(number) - number of hidden notes of the network\n",
    "            \n",
    "        '''\n",
    "        self.n_hidden_nodes=n_hidden_nodes\n",
    "        self.n_features=40000\n",
    "        self.n_labels=len(self.labels_one_hot[0])\n",
    "        \n",
    "        self.x=tf.placeholder(dtype=tf.float32,shape=[None,self.n_features])\n",
    "        self.y=tf.placeholder(dtype=tf.float32,shape=[None,self.n_labels])\n",
    "        \n",
    "        self.w1=tf.Variable(tf.truncated_normal([self.n_features,self.n_hidden_nodes]))\n",
    "        self.w2=tf.Variable(tf.truncated_normal([self.n_hidden_nodes,self.n_labels]))\n",
    "        self.b1=tf.Variable(tf.zeros([self.n_hidden_nodes]))\n",
    "        self.b2=tf.Variable(tf.zeros([self.n_labels]))\n",
    "        \n",
    "        h1=tf.matmul(self.x,self.w1)+self.b1\n",
    "        a1=tf.nn.relu(h1)\n",
    "        \n",
    "        h2=tf.matmul(a1,self.w2)+self.b2\n",
    "        a2=tf.nn.relu(h2)\n",
    "        \n",
    "        self.output=tf.nn.softmax(a2)\n",
    "        self.prediction = tf.argmax(self.output,1)\n",
    "        self.correct_prediction = tf.equal(self.prediction, tf.argmax(self.y, 1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(self.correct_prediction, tf.float32))\n",
    "        \n",
    "        self.cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=self.output,labels=self.y))\n",
    "        \n",
    "    def train_model(self,learning_rate,epochs,batch_size,test_size=0.2):\n",
    "        '''Method which trains the model\n",
    "            Args:\n",
    "                learning_rate(number) - learning rate used in gradient descent\n",
    "                epochs(number) - number of iterations (epochs) used in gradient descent\n",
    "                batch_size(number) - size of the mini batches used in training\n",
    "                test_size(number) - size of the test set\n",
    "        '''\n",
    "        self.lr=learning_rate\n",
    "        self.optimizer=tf.train.GradientDescentOptimizer(learning_rate=self.lr).minimize(self.cost)\n",
    "        X_train,X_test,y_train,y_test=self.get_train_test_split(test_size)\n",
    "        batches=self.get_batches(batch_size)\n",
    "\n",
    "        init = tf.global_variables_initializer()\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init)\n",
    "            \n",
    "            for epoch in range(epochs):\n",
    "                for X_batch,y_batch in batches:\n",
    "                    sess.run(self.optimizer,feed_dict={self.x:X_batch,self.y:y_batch})\n",
    "                cost_train=sess.run(self.cost,feed_dict={self.x:X_train,self.y:y_train})\n",
    "                accuracy_test=sess.run(self.accuracy,feed_dict={self.x:X_test,self.y:y_test})\n",
    "                accuracy_train=sess.run(self.accuracy,feed_dict={self.x:X_train,self.y:y_train})\n",
    "                print(\"In epoch {} is the cost equals {}\".format(epoch,cost_train))\n",
    "                print(\"In epoch {} is the accuracy on the training set equals {}\".format(epoch,accuracy_train))\n",
    "                print(\"In epoch {} is the accuracy on the test set equals {}\".format(epoch,accuracy_test)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word TFIDF\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"char_vectorizer = TfidfVectorizer(\\n    sublinear_tf=True,\\n    strip_accents='unicode',\\n    analyzer='char',\\n    ngram_range=(1, 4),\\n    max_features=40000)\\nchar_vectorizer.fit(all_text)\\ntrain_char_features = char_vectorizer.transform(train_text)\\ntest_char_features = char_vectorizer.transform(test_text)\\nprint('char TFIDF')\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pickle.load(open('train.p','rb'))\n",
    "test = pickle.load(open('test.p','rb'))\n",
    "\n",
    "train_text = train['comment_text']\n",
    "test_text = test\n",
    "all_text = pd.concat([train_text, test_text])\n",
    "\n",
    "word_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    token_pattern=r'\\w{1,}',\n",
    "    ngram_range=(1, 4),\n",
    "    max_features=40000)\n",
    "word_vectorizer.fit(all_text)\n",
    "train_word_features = word_vectorizer.transform(train_text)\n",
    "test_word_features = word_vectorizer.transform(test_text)\n",
    "print('word TFIDF')\n",
    "\n",
    "'''char_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='char',\n",
    "    ngram_range=(1, 4),\n",
    "    max_features=40000)\n",
    "char_vectorizer.fit(all_text)\n",
    "train_char_features = char_vectorizer.transform(train_text)\n",
    "test_char_features = char_vectorizer.transform(test_text)\n",
    "print('char TFIDF')'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<159571x40000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 14083710 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features=train_word_features\n",
    "labels = ['toxic','severe_toxic','obscene','threat','insult','identity_hate','clean']\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "network=NeuralNetwork(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "network.set_labels(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.build_neural_net(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 39810)\t0.132426083394\n",
      "  (0, 38674)\t0.118646594328\n",
      "  (0, 38606)\t0.0720239554381\n",
      "  (0, 37803)\t0.115209148362\n",
      "  (0, 37549)\t0.0953624053108\n",
      "  (0, 37453)\t0.120503226774\n",
      "  (0, 36974)\t0.150316844476\n",
      "  (0, 36963)\t0.0916308549876\n",
      "  (0, 34228)\t0.129325790864\n",
      "  (0, 34213)\t0.108455589355\n",
      "  (0, 34212)\t0.106938262379\n",
      "  (0, 33110)\t0.105994943377\n",
      "  (0, 32601)\t0.0616612247364\n",
      "  (0, 31879)\t0.173571140895\n",
      "  (0, 31872)\t0.105219812606\n",
      "  (0, 31730)\t0.0780626377645\n",
      "  (0, 31715)\t0.0642369228001\n",
      "  (0, 30328)\t0.0665750210564\n",
      "  (0, 30014)\t0.126745521365\n",
      "  (0, 30013)\t0.104186004288\n",
      "  (0, 30005)\t0.0841346115241\n",
      "  (0, 28690)\t0.112377445164\n",
      "  (0, 28666)\t0.0866868354711\n",
      "  (0, 28645)\t0.153010188623\n",
      "  (0, 28410)\t0.100287767968\n",
      "  :\t:\n",
      "  (159570, 15528)\t0.151561627548\n",
      "  (159570, 15324)\t0.0662604535785\n",
      "  (159570, 14868)\t0.17887046985\n",
      "  (159570, 14863)\t0.120196039822\n",
      "  (159570, 14859)\t0.0728076336854\n",
      "  (159570, 14770)\t0.088156210304\n",
      "  (159570, 14130)\t0.0428512295895\n",
      "  (159570, 13647)\t0.167589119976\n",
      "  (159570, 13610)\t0.16153158337\n",
      "  (159570, 13599)\t0.122064524448\n",
      "  (159570, 10563)\t0.134877535959\n",
      "  (159570, 10561)\t0.114093431765\n",
      "  (159570, 10373)\t0.0558769963617\n",
      "  (159570, 10319)\t0.0460360925137\n",
      "  (159570, 8519)\t0.119696173698\n",
      "  (159570, 8407)\t0.160626980544\n",
      "  (159570, 8392)\t0.0885234424051\n",
      "  (159570, 4706)\t0.164398740119\n",
      "  (159570, 4373)\t0.0287260536901\n",
      "  (159570, 4224)\t0.186723389003\n",
      "  (159570, 4212)\t0.188351719901\n",
      "  (159570, 4120)\t0.197928147534\n",
      "  (159570, 2678)\t0.074326007383\n",
      "  (159570, 2512)\t0.0888533009702\n",
      "  (159570, 2302)\t0.0642050014797 [[0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [159571, 7]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-f3d0e06c9c8a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.9\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-43-a9669b8bedc7>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(self, learning_rate, epochs, batch_size, test_size)\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGradientDescentOptimizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcost\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m         \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_train_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m         \u001b[0mbatches\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_batches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-43-a9669b8bedc7>\u001b[0m in \u001b[0;36mget_train_test_split\u001b[1;34m(self, test_size)\u001b[0m\n\u001b[0;32m     25\u001b[0m         '''\n\u001b[0;32m     26\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels_one_hot\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels_one_hot\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[1;34m(*arrays, **options)\u001b[0m\n\u001b[0;32m   2029\u001b[0m         \u001b[0mtest_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.25\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2030\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2031\u001b[1;33m     \u001b[0marrays\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2032\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2033\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mindexable\u001b[1;34m(*iterables)\u001b[0m\n\u001b[0;32m    227\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m             \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 229\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    230\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    202\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[1;32m--> 204\u001b[1;33m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[0;32m    205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [159571, 7]"
     ]
    }
   ],
   "source": [
    "network.train_model(learning_rate=0.9,epochs=7,batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<159571x40000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 14083710 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.features#,self.labels_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.labels_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ola.alnaameh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\utils.py:1167: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import sys, os, re, csv, codecs, numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D,Bidirectional\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import gensim.models.keyedvectors as word2vec\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pickle.load(open('train.p','rb'))\n",
    "test = pickle.load(open('test.p','rb'))\n",
    "\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values\n",
    "list_sentences_train = train[\"comment_text\"]\n",
    "list_sentences_test = test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 20000\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maxlen = 200\n",
    "X_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_te = pad_sequences(list_tokenized_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadEmbeddingMatrix(typeToLoad):\n",
    "        #load different embedding file from Kaggle depending on which embedding \n",
    "        #matrix we are going to experiment with\n",
    "        if(typeToLoad==\"glove\"):\n",
    "            EMBEDDING_FILE='../input/glove-twitter/glove.twitter.27B.25d.txt'\n",
    "            embed_size = 25\n",
    "        elif(typeToLoad==\"word2vec\"):\n",
    "            word2vecDict = word2vec.KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin\", binary=True)\n",
    "            embed_size = 300\n",
    "        elif(typeToLoad==\"fasttext\"):\n",
    "            EMBEDDING_FILE='../input/fasttext/wiki.simple.vec'\n",
    "            embed_size = 300\n",
    "\n",
    "        if(typeToLoad==\"glove\" or typeToLoad==\"fasttext\" ):\n",
    "            embeddings_index = dict()\n",
    "            #Transfer the embedding weights into a dictionary by iterating through every line of the file.\n",
    "            f = open(EMBEDDING_FILE)\n",
    "            for line in f:\n",
    "                #split up line into an indexed array\n",
    "                values = line.split()\n",
    "                #first index is word\n",
    "                word = values[0]\n",
    "                #store the rest of the values in the array as a new array\n",
    "                coefs = np.asarray(values[1:], dtype='float32')\n",
    "                embeddings_index[word] = coefs #50 dimensions\n",
    "            f.close()\n",
    "            print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "        else:\n",
    "            embeddings_index = dict()\n",
    "            for word in word2vecDict.wv.vocab:\n",
    "                embeddings_index[word] = word2vecDict.word_vec(word)\n",
    "            print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "            \n",
    "        gc.collect()\n",
    "        #We get the mean and standard deviation of the embedding weights so that we could maintain the \n",
    "        #same statistics for the rest of our own random generated weights. \n",
    "        all_embs = np.stack(list(embeddings_index.values()))\n",
    "        emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "        \n",
    "        nb_words = len(tokenizer.word_index)\n",
    "        #We are going to set the embedding size to the pretrained dimension as we are replicating it.\n",
    "        #the size will be Number of Words in Vocab X Embedding Size\n",
    "        embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "        gc.collect()\n",
    "\n",
    "        #With the newly created embedding matrix, we'll fill it up with the words that we have in both \n",
    "        #our own dictionary and loaded pretrained embedding. \n",
    "        embeddedCount = 0\n",
    "        for word, i in tokenizer.word_index.items():\n",
    "            i-=1\n",
    "            #then we see if this word is in glove's dictionary, if yes, get the corresponding weights\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "            #and store inside the embedding matrix that we will train later on.\n",
    "            if embedding_vector is not None: \n",
    "                embedding_matrix[i] = embedding_vector\n",
    "                embeddedCount+=1\n",
    "        print('total embedded:',embeddedCount,'common words')\n",
    "        \n",
    "        del(embeddings_index)\n",
    "        gc.collect()\n",
    "        \n",
    "        #finally, return the embedding matrix\n",
    "        return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-70-18657f50a574>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0membedding_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloadEmbeddingMatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'word2vec'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-69-1e6232af2c39>\u001b[0m in \u001b[0;36mloadEmbeddingMatrix\u001b[1;34m(typeToLoad)\u001b[0m\n\u001b[0;32m      6\u001b[0m             \u001b[0membed_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m25\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[1;32melif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtypeToLoad\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m\"word2vec\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m             \u001b[0mword2vecDict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword2vec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mKeyedVectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"GoogleNews-vectors-negative300.bin\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m             \u001b[0membed_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;32melif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtypeToLoad\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m\"fasttext\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[0;32m   1001\u001b[0m         return _load_word2vec_format(\n\u001b[0;32m   1002\u001b[0m             \u001b[0mWord2VecKeyedVectors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1003\u001b[1;33m             limit=limit, datatype=datatype)\n\u001b[0m\u001b[0;32m   1004\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1005\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_keras_embedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_embeddings\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\models\\utils_any2vec.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[0;32m    167\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msmart_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m         \u001b[0mheader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m         \u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvector_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# throws for invalid file format\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlimit\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m             \u001b[0mvocab_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 0)"
     ]
    }
   ],
   "source": [
    "embedding_matrix = loadEmbeddingMatrix('word2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
