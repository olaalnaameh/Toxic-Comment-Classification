{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed Forward Neural Network\n",
    "This Notebook contains the definition of a class, which implements a feed forward neural network. This network can be used to solve a classification problem (e.g. the classification of text documents). When instantiating an object of the class, the features must be specified. Afterwards, the labels must be set. Internally, the class will provide a one-hot encoding of the labels. Further, the class contains a method which implements the splitting of the data in a training set and a testing set. Also, the size of batches used to perform (stochastic) gradient descent, can be specified, as well as the number of hidden nodes used by the network. The output of the network is a probability distribution over the set of labels. The output is generated by applying softmax on the output of the hidden layer. The non-linearity used by the hidden layer is given by the ReLu function i.e.\n",
    "$$f(x)=\\max\\{0,x\\}$$\n",
    "The cost function is defined in terms of the cross-entropy of the output and the true training sample:\n",
    "$$ Cost(y,\\hat{y})= -\\sum_iy_i\\ln(\\hat{y_1})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loading of the relevant packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of the Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class NeuralNetwork(object):\n",
    "    def __init__(self,features):\n",
    "        '''Constructor of the class\n",
    "            Args:\n",
    "                features(matrix) - feature matrix where the columns describe the features and the rows the samples\n",
    "        '''\n",
    "        self.features=features\n",
    "        \n",
    "    def set_labels(self,labels):\n",
    "        '''Method which sets the labels, i.e. the targets we want to predict based on the features\n",
    "            Args:\n",
    "                labels(vector) - vector where the i-th element is the label which corresponds to the i-th row of the features\n",
    "                matrix\n",
    "        '''\n",
    "        self.labels=labels\n",
    "        # the code below implements one-hot encoding of the labels\n",
    "        lb=LabelBinarizer()\n",
    "        lb.fit([label[0] for label in labels])\n",
    "        self.labels_one_hot=lb.transform(labels)\n",
    "\n",
    "    def get_train_test_split(self,test_size=0.2):\n",
    "        '''Method which splits the features and labels in a training set and a testing set\n",
    "            Args:\n",
    "                test_size(number) - defines the size of the test set\n",
    "        '''\n",
    "        X_train,X_test,y_train,y_test=\\\n",
    "        train_test_split(self.features,self.labels_one_hot,test_size=test_size)\n",
    "        return (X_train,X_test,y_train,y_test)\n",
    "        \n",
    "    def get_batches(self,batch_size):\n",
    "        '''Helper method to devide the features and labels in mini-batches\n",
    "            Args:\n",
    "                batch_size(number) - size of the batches e.g. 256 (should fit in memory of the machine)\n",
    "        '''\n",
    "        assert len(self.features) == len(self.labels)\n",
    "        output_batches = []\n",
    "    \n",
    "        sample_size = len(self.features)\n",
    "        for start_i in range(0, sample_size, batch_size):\n",
    "            end_i = start_i + batch_size\n",
    "            batch = [self.features[start_i:end_i], self.labels_one_hot[start_i:end_i]]\n",
    "            output_batches.append(batch)\n",
    "        \n",
    "        return output_batches\n",
    "    \n",
    "    def build_neural_net(self,n_hidden_nodes):\n",
    "        '''Method which builds the Neuronal Network in TensorFlow\n",
    "            Args:\n",
    "                n_hidden_nodes(number) - number of hidden notes of the network\n",
    "            \n",
    "        '''\n",
    "        self.n_hidden_nodes=n_hidden_nodes\n",
    "        self.n_features=len(self.features[0])\n",
    "        self.n_labels=len(self.labels_one_hot[0])\n",
    "        \n",
    "        self.x=tf.placeholder(dtype=tf.float32,shape=[None,self.n_features])\n",
    "        self.y=tf.placeholder(dtype=tf.float32,shape=[None,self.n_labels])\n",
    "        \n",
    "        self.w1=tf.Variable(tf.truncated_normal([self.n_features,self.n_hidden_nodes]))\n",
    "        self.w2=tf.Variable(tf.truncated_normal([self.n_hidden_nodes,self.n_labels]))\n",
    "        self.b1=tf.Variable(tf.zeros([self.n_hidden_nodes]))\n",
    "        self.b2=tf.Variable(tf.zeros([self.n_labels]))\n",
    "        \n",
    "        h1=tf.matmul(self.x,self.w1)+self.b1\n",
    "        a1=tf.nn.relu(h1)\n",
    "        \n",
    "        h2=tf.matmul(a1,self.w2)+self.b2\n",
    "        a2=tf.nn.relu(h2)\n",
    "        \n",
    "        self.output=tf.nn.softmax(a2)\n",
    "        self.prediction = tf.argmax(self.output,1)\n",
    "        self.correct_prediction = tf.equal(self.prediction, tf.argmax(self.y, 1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(self.correct_prediction, tf.float32))\n",
    "        \n",
    "        self.cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=self.output,labels=self.y))\n",
    "        \n",
    "    def train_model(self,learning_rate,epochs,batch_size,test_size=0.2):\n",
    "        '''Method which trains the model\n",
    "            Args:\n",
    "                learning_rate(number) - learning rate used in gradient descent\n",
    "                epochs(number) - number of iterations (epochs) used in gradient descent\n",
    "                batch_size(number) - size of the mini batches used in training\n",
    "                test_size(number) - size of the test set\n",
    "        '''\n",
    "        self.lr=learning_rate\n",
    "        self.optimizer=tf.train.GradientDescentOptimizer(learning_rate=self.lr).minimize(self.cost)\n",
    "        X_train,X_test,y_train,y_test=self.get_train_test_split(test_size)\n",
    "        batches=self.get_batches(batch_size)\n",
    "\n",
    "        init = tf.global_variables_initializer()\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init)\n",
    "            \n",
    "            for epoch in range(epochs):\n",
    "                for X_batch,y_batch in batches:\n",
    "                    sess.run(self.optimizer,feed_dict={self.x:X_batch,self.y:y_batch})\n",
    "                cost_train=sess.run(self.cost,feed_dict={self.x:X_train,self.y:y_train})\n",
    "                accuracy_test=sess.run(self.accuracy,feed_dict={self.x:X_test,self.y:y_test})\n",
    "                accuracy_train=sess.run(self.accuracy,feed_dict={self.x:X_train,self.y:y_train})\n",
    "                print(\"In epoch {} is the cost equals {}\".format(epoch,cost_train))\n",
    "                print(\"In epoch {} is the accuracy on the training set equals {}\".format(epoch,accuracy_train))\n",
    "                print(\"In epoch {} is the accuracy on the test set equals {}\".format(epoch,accuracy_test)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Illustration\n",
    "The cells below show by using toy data how the class behaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features=[[1,2,3],\n",
    "          [6,6,6],\n",
    "          [8,8,8],\n",
    "          [4.6,6,7.9],\n",
    "          [4,4,4],\n",
    "          [3,4,2]]\n",
    "labels=[[3],\n",
    "        [7],\n",
    "        [9],\n",
    "        [3],\n",
    "        [1],\n",
    "        [2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "network=NeuralNetwork(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "network.set_labels(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "network.build_neural_net(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In epoch 0 is the cost equals 1.605173110961914\n",
      "In epoch 0 is the accuracy on the training set equals 0.5\n",
      "In epoch 0 is the accuracy on the test set equals 0.0\n",
      "In epoch 1 is the cost equals 1.6000416278839111\n",
      "In epoch 1 is the accuracy on the training set equals 0.5\n",
      "In epoch 1 is the accuracy on the test set equals 0.0\n",
      "In epoch 2 is the cost equals 1.5952329635620117\n",
      "In epoch 2 is the accuracy on the training set equals 0.5\n",
      "In epoch 2 is the accuracy on the test set equals 0.0\n",
      "In epoch 3 is the cost equals 1.5902115106582642\n",
      "In epoch 3 is the accuracy on the training set equals 0.5\n",
      "In epoch 3 is the accuracy on the test set equals 0.0\n",
      "In epoch 4 is the cost equals 1.5849881172180176\n",
      "In epoch 4 is the accuracy on the training set equals 0.5\n",
      "In epoch 4 is the accuracy on the test set equals 0.0\n",
      "In epoch 5 is the cost equals 1.5795880556106567\n",
      "In epoch 5 is the accuracy on the training set equals 0.5\n",
      "In epoch 5 is the accuracy on the test set equals 0.0\n",
      "In epoch 6 is the cost equals 1.5740443468093872\n",
      "In epoch 6 is the accuracy on the training set equals 0.5\n",
      "In epoch 6 is the accuracy on the test set equals 0.0\n"
     ]
    }
   ],
   "source": [
    "network.train_model(learning_rate=0.9,epochs=7,batch_size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Next Steps\n",
    "The model can easily extended to include more hidden layers (i.e. we can make the model deeper). Further, the cost function can be adjusted to reflect class imbalance and dropout can be included to avoid overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
