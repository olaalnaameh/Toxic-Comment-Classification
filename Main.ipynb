{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def RemovePunc(comment):\n",
    "    comment = comment.replace('^\\d+|\\n|\\t|\"|==|;|:|@', '')\n",
    "    comment = comment.replace('\\d+', '')\n",
    "    comment = comment.replace(',', '')\n",
    "    comment = comment.replace('?', '')\n",
    "    comment = comment.replace('!', '')\n",
    "    comment = comment.replace('&', '')\n",
    "    comment = comment.replace('[...]', '')\n",
    "    comment = comment.replace('[....]', '')\n",
    "    for c in string.punctuation:\n",
    "        comment = comment.replace(c, \"\")\n",
    "    return (comment)\n",
    "\n",
    "def RemoveHttp(comment):\n",
    "    comment = re.sub(r'(((https|http)?://)|(www.))(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F])|#)',\n",
    "                     '', comment, flags=re.MULTILINE)\n",
    "    return (comment)\n",
    "\n",
    "def ConvertToLowerCase(comment):\n",
    "    comment = comment.lower()\n",
    "    return(comment)\n",
    "\n",
    "def StemComment(comment,lowerCase,punc,http):\n",
    "    if lowerCase:\n",
    "        comment = ConvertToLowerCase(comment)\n",
    "    if punc:\n",
    "        comment = RemovePunc(comment)\n",
    "    if http:\n",
    "        comment = RemoveHttp(comment)\n",
    "        \n",
    "    comment=''.join([i for i in comment if not i.isdigit()])\n",
    "    comment = nlp(comment)\n",
    "    lemmatized = []\n",
    "    for word in comment:\n",
    "        lemma = word.lemma_.strip()\n",
    "        if lemma:\n",
    "            lemmatized.append(lemma)\n",
    "    return \" \".join(lemmatized)\n",
    "    \n",
    "def dataStem(Data,lowerCase,punc,http):\n",
    "    Data_lemmatized = Data.apply(StemComment, lowerCase=True,punc=False,http=True)\n",
    "    return Data_lemmatized\n",
    "\n",
    "def saveInFile(x_train_lemmatized,fileName):\n",
    "    filehandler = open(fileName+\".p\", \"wb\")\n",
    "    pickle.dump(x_train_lemmatized, filehandler)\n",
    "    filehandler.close()\n",
    "    \n",
    "def loadFromFile(fileName):\n",
    "    file = open(fileName+\".p\", 'rb')\n",
    "    object_file = pickle.load(file)\n",
    "    file.close()\n",
    "    return object_file\n",
    "\n",
    "def sampling(data):\n",
    "    clean = data.loc[np.random.choice(data[data['clean']==True].index.tolist(),size=9237)].reset_index(drop=True)\n",
    "    severe_toxic = data.loc[np.random.choice(data[data['severe_toxic']==True].index.tolist(),size=6000,replace=True)].reset_index(drop=True)\n",
    "    obscene = data.loc[np.random.choice(data[data['obscene']==True].index.tolist(),size=2000,replace=True)].reset_index(drop=True)\n",
    "    threat = data.loc[np.random.choice(data[data['threat']==True].index.tolist(),size=6000,replace=True)].reset_index(drop=True)\n",
    "    insult = data.loc[np.random.choice(data[data['insult']==True].index.tolist(),size=2300,replace=True)].reset_index(drop=True)\n",
    "    identity_hate = data.loc[np.random.choice(data[data['identity_hate']==True].index.tolist(),size=6000,replace=True)].reset_index(drop=True)\n",
    "    toxic = data[data['toxic']==1].reset_index(drop=True)\n",
    "\n",
    "    sampled_train = pd.concat([clean,threat,toxic,severe_toxic,insult,identity_hate,obscene]).reset_index(drop=True)\n",
    "    sampled_train = sampled_train.sample(frac=1).reset_index(drop=True)\n",
    "    sampled_train['clean'] = sampled_train['clean'].astype(int)\n",
    "    return sampled_train\n",
    "\n",
    "def Translate(data):\n",
    "    translator = Translator()\n",
    "    translations= translator.translate(data)\n",
    "    return translations.text\n",
    "\n",
    "def TranslateTheDataSet(data):\n",
    "    for index, row in data.iterrows():\n",
    "        rowString=row['comment_text']\n",
    "        if(len(rowString)>5000):\n",
    "            rowString=row['comment_text'][:5000]\n",
    "        try:\n",
    "            translate = Translator()\n",
    "            lang = translate.detect(rowString).lang\n",
    "        except ValueError:\n",
    "            continue\n",
    "        if (lang != 'en'):\n",
    "            try:\n",
    "                transComment = Translate(rowString)\n",
    "                data.at[index, 'comment_text'] = transComment\n",
    "            except ValueError:\n",
    "                continue\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply Stemming on Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start train lemmatize\n",
      "id\n",
      "0000997932d777bf    explanation why the edit make under -PRON- use...\n",
      "000103f0d9cfb60f    d'aww ! -PRON- match this background colour -P...\n",
      "000113f07ec002fd    hey man , -PRON- be really not try to edit war...\n",
      "0001b41b1c6bb37e    \" more i can not make any real suggestion on i...\n",
      "0001d958c54c6e35    -PRON- , sir , be -PRON- hero . any chance -PR...\n",
      "00025465d4725e87    \" congratulation from -PRON- as well , use the...\n",
      "0002bcb3da6cb337    cocksucker before -PRON- pis around on -PRON- ...\n",
      "00031b1e95af7921    -PRON- vandalism to the matt shirvington artic...\n",
      "00037261f536c51d    sorry if the word ' nonsense ' be offensive to...\n",
      "00040093b2687caa    alignment on this subject and which be contrar...\n",
      "0005300084f90edc    \" fair use rationale for image : wonju.jpg tha...\n",
      "00054a5e18b50dd4    bbq be a man and let discus -PRON- - maybe ove...\n",
      "0005c987bdfc9d4b    hey ... what be -PRON- .. @ | talk . what be -...\n",
      "0006f16e4e9f292e    before -PRON- start throw accusation and warni...\n",
      "00070ef96486d6f9    oh , and the girl above start -PRON- argument ...\n",
      "00078f8ce7eb276d    \" juelz santana age in , juelz santana be year...\n",
      "0007e25b2121310b    bye ! do not look , come or think of comm back...\n",
      "000897889268bc93    redirect talk : voydan pop georgiev- chernodri...\n",
      "0009801bd85e5806    the mitsurugi point make no sense - why not ar...\n",
      "0009eaea3325de8c    do not mean to bother -PRON- i see that -PRON-...\n",
      "000b08c464718505    \" regard -PRON- recent edit once again , pleas...\n",
      "000bfd0867774845    \" good to know . about -PRON- , yeah , -PRON- ...\n",
      "000c0dfd995809fa    \" snowflake be not always symmetrical ! under ...\n",
      "000c6a3f0cd3ba8e    \" the signpost : september read this signpost ...\n",
      "000cfee90f50d471    \" re - consider st paragraph edit ? i do not u...\n",
      "000eefc67a2c930f    radial symmetry several now extinct lineage in...\n",
      "000f35deef84dc4a    there be no need to apologize . a wikipedia ar...\n",
      "000ffab30195c5e1    yes , because the mother of the child in the c...\n",
      "0010307a3a50a353    \" ok . but -PRON- will take a bit of work but ...\n",
      "0010833a96e1f886    \" = = a barnstar for -PRON- ! = = the real lif...\n",
      "                                          ...                        \n",
      "ffa33d3122b599d6    -PRON- absurd edit -PRON- absurd edit on great...\n",
      "ffa95244f261527f    maybe -PRON- be get good thing to do than spen...\n",
      "ffad104337fe9891    scrap that , -PRON- do meet criterion and -PRO...\n",
      "ffaed63c487a2b42                                -PRON- could do bad .\n",
      "ffb268f37788a011    , march ( utc ) be -PRON- also user : bmattson...\n",
      "ffb47123b2d82762    \" hey listen do not -PRON- ever ! ! ! ! delete...\n",
      "ffb7b4c3d3ae5842                   thank -PRON- very , very much . ·✆\n",
      "ffb93b0a0a1e78f9                                 talkback : september\n",
      "ffb998f9749bd83e                                      ( utc ) : , mar\n",
      "ffba5332d6b8fd14    i agree/ on another note lil wayne be a talent...\n",
      "ffbc2db4225258dd    while about half the reference be from byu - i...\n",
      "ffbcd64a71775e04    prague spring i think that prague spring deser...\n",
      "ffbd331a3aa269b9    i see this as have be merge ; undo one side of...\n",
      "ffbdbb0483ed0841    and -PRON- be go to keep post the stuff u dele...\n",
      "ffc2f409658571f1    \" how come when -PRON- download that mp -PRON-...\n",
      "ffc671f2acdd80e1    -PRON- will be on irc , too , if -PRON- have a...\n",
      "ffc7bbb177c3c966    -PRON- be -PRON- opinion that that happen to b...\n",
      "ffca1e81aefc48ac    please stop remove content from wikipedia ; -P...\n",
      "ffca8d71d71a3fae    image : barack - obama - mother.jpg list for d...\n",
      "ffcdcb71854f6d8a    \" editing of article without consensus & remov...\n",
      "ffd2e85b07b3c7e4    \" no -PRON- do not , read -PRON- again ( i wou...\n",
      "ffd72e9766c09c97    \" auto guide and the motoring press be not goo...\n",
      "ffe029a7c79dc7fe    \" please identify what part of blp apply becau...\n",
      "ffe897e7f7182c90    catalan independentism be the social movement ...\n",
      "ffe8b9316245be30    the number in parenthesis be the additional de...\n",
      "ffe987279560d7ff    \" : : : : : and for the second time of ask , w...\n",
      "ffea4adeee384e90    -PRON- should be ashamed of -PRON- that be a h...\n",
      "ffee36eab5c267c9    spitzer umm , there s no actual article for pr...\n",
      "fff125370e4aaaf3    and -PRON- look like -PRON- be actually -PRON-...\n",
      "fff46fc426af1f9a    \" and ... i really do not think -PRON- underst...\n",
      "Name: comment_text, Length: 159571, dtype: object\n",
      "start test lemmatize\n",
      "Lemmatizing and storing done!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import pickle\n",
    "import re\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "\n",
    "\n",
    "#nlp = spacy.load(\"en\", disable=['parser', 'tagger', 'ner'])\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "train = pd.read_csv('New_Data\\\\train.csv')\n",
    "test = pd.read_csv('New_Data\\\\test.csv')\n",
    "#print(train)\n",
    "train.index = train['id']\n",
    "x_train = train['comment_text']\n",
    "y_train = train.iloc[:, 2:]\n",
    "test.index = test['id']\n",
    "x_test = test['comment_text']\n",
    "y_train['clean'] = 1 - y_train.sum(axis=1) >= 1\n",
    "x_train2 = x_train.dropna()\n",
    "x_test2 = x_test.dropna()\n",
    "print('start train lemmatize')\n",
    "x_train_lemmatized=dataStem(x_train2,lowerCase=True,punc=False,http=True)\n",
    "print(x_train_lemmatized)\n",
    "print('start test lemmatize')\n",
    "x_test_lemmatized=dataStem(x_test2,lowerCase=True,punc=False,http=True)\n",
    "train2=pd.concat([x_train_lemmatized,y_train], axis=1)\n",
    "\n",
    "saveInFile(train2,'train_TestwithPunc')\n",
    "saveInFile(x_test_lemmatized,'test_TestwithPunc')\n",
    "print('Lemmatizing and storing done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# NB with 50,000 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (2, 1208)\t0.550168536241\n",
      "  (2, 0)\t0.575755963143\n",
      "  (2, 1209)\t0.604830267625\n",
      "  (7, 507)\t0.600881067348\n",
      "  (7, 865)\t0.527018888054\n",
      "  (7, 511)\t0.600993373122\n",
      "  (9, 1208)\t0.0807629536639\n",
      "  (9, 0)\t0.0845191048016\n",
      "  (9, 1209)\t0.0887871182393\n",
      "  (9, 507)\t0.261420587444\n",
      "  (9, 865)\t0.229285951573\n",
      "  (9, 511)\t0.261469447431\n",
      "  (9, 1928)\t0.173002750312\n",
      "  (9, 927)\t0.128358494823\n",
      "  (9, 1134)\t0.224917152186\n",
      "  (9, 1944)\t0.235741213389\n",
      "  (9, 972)\t0.123350035109\n",
      "  (9, 526)\t0.139420143773\n",
      "  (9, 930)\t0.128466808371\n",
      "  (9, 577)\t0.233344042482\n",
      "  (9, 1139)\t0.286499209397\n",
      "  (9, 1945)\t0.235741213389\n",
      "  (9, 538)\t0.137389506148\n",
      "  (9, 973)\t0.13488691332\n",
      "  (9, 527)\t0.139420143773\n",
      "  :\t:\n",
      "  (46820, 1)\t0.287994627975\n",
      "  (46820, 1210)\t0.287994627975\n",
      "  (46820, 2)\t0.287994627975\n",
      "  (46820, 1211)\t0.287994627975\n",
      "  (46820, 120)\t0.340715236578\n",
      "  (46820, 8)\t0.341352511923\n",
      "  (46821, 1208)\t0.17999180935\n",
      "  (46821, 507)\t0.19420440685\n",
      "  (46821, 865)\t0.170332194032\n",
      "  (46821, 511)\t0.19424070401\n",
      "  (46821, 972)\t0.274903219799\n",
      "  (46821, 538)\t0.306192191784\n",
      "  (46821, 2066)\t0.319642015483\n",
      "  (46821, 1467)\t0.378362265765\n",
      "  (46821, 1031)\t0.465916545007\n",
      "  (46821, 549)\t0.481053429273\n",
      "  (46826, 1597)\t0.382249736969\n",
      "  (46826, 1752)\t0.481187351792\n",
      "  (46826, 1710)\t0.788887743004\n",
      "  (46829, 507)\t0.600881067348\n",
      "  (46829, 865)\t0.527018888054\n",
      "  (46829, 511)\t0.600993373122\n",
      "  (46830, 507)\t0.600881067348\n",
      "  (46830, 865)\t0.527018888054\n",
      "  (46830, 511)\t0.600993373122\n",
      "toxic\n",
      "[ 0.63096739  0.77973992  0.77973992 ...,  0.77973992  0.77045284\n",
      "  0.89822904]\n",
      "(153164, 7) (153164,)\n",
      "severe_toxic\n",
      "[ 0.47709688  0.23689436  0.23689436 ...,  0.23689436  0.1572914\n",
      "  0.25685231]\n",
      "(153164, 7) (153164,)\n",
      "obscene\n",
      "[ 0.30318143  0.54837608  0.54837608 ...,  0.54837608  0.49327793\n",
      "  0.64741227]\n",
      "(153164, 7) (153164,)\n",
      "threat\n",
      "[ 0.1699552   0.158378    0.158378   ...,  0.158378    0.0231251\n",
      "  0.16579094]\n",
      "(153164, 7) (153164,)\n",
      "insult\n",
      "[ 0.28885323  0.53633277  0.53633277 ...,  0.53633277  0.47120743\n",
      "  0.65464599]\n",
      "(153164, 7) (153164,)\n",
      "identity_hate\n",
      "[ 0.30092611  0.22068715  0.22068715 ...,  0.22068715  0.05557842\n",
      "  0.21110858]\n",
      "(153164, 7) (153164,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "train = pickle.load(open('train_Test.p','rb'))\n",
    "test = pickle.load(open('test_Test.p','rb'))\n",
    "sample = pd.read_csv('sample_submission.csv')\n",
    "sampled_train=sampling(train)\n",
    "feature_model = TfidfVectorizer(ngram_range=(1, 4),max_features=50000)\n",
    "feature_matrix = feature_model.fit_transform(sampled_train['comment_text'])\n",
    "print(feature_matrix)\n",
    "test_x = feature_model.transform(test)\n",
    "for col in ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']:\n",
    "    clf = MultinomialNB()\n",
    "    clf.fit(feature_matrix, sampled_train[col])\n",
    "    preds = clf.predict_proba(test_x)[:, 1]\n",
    "    print(col)\n",
    "    print(preds)\n",
    "    print(sample.shape, preds.shape)\n",
    "    sample[col] = preds\n",
    "\n",
    "sample.to_csv(\"result0.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
